# -*- coding: utf-8 -*-
"""Text-Generation with RNN.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Aq7Ji01G4nEjJLuyY-hAyC1zvab11YLg

### Generating Text from a Character level using RNN

* Article : [Andrej_Karpathy](http://karpathy.github.io/2015/05/21/rnn-effectiveness/)
"""

# Commented out IPython magic to ensure Python compatibility.
# %tensorflow_version 2.x

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
# %matplotlib inline
import tensorflow as tf

"""-------------------------------------------------------------------------------------------------------------------------------

### 1) Data

* We can grab any free text from here : [text](https://www.gutenberg.org/)


* We will choose Shakespeare's works for two main reasons :

    1) Its a large corpus of text, its usually recommended you have at least a source of 1 million characters total to get realistic text generation

    2) It has a very distinctive style. Since the text data uses old style english and is formatted in the style of a stage play, it will be very obvious to us if the model is able to reproduce similar results.
"""

path_to_file = 'shakespeare.txt'

text = open(path_to_file, 'r').read()    # read it with mode 'r'

print(text[:500])

"""### `Understanding the Unique Characters`"""

vocab = sorted(set(text))

print(vocab)

len(vocab)            # We need to remember this for the last Dense layer

"""-------------------------------------------------------------------------------------------------------------------------------

### 2) Text Processing

* `Text Vectorization`

* `Create Encoding Dictionary`

**We know a neural network can't take in the raw_string data, we need to assign numbers to each character**

**Let's create two dictionaries that can go from numeric index to character and character to numeric index**

##### We will use `enumerate logic` : which creates a tuple containing the integer or number for corresponding characters
"""

for pair in enumerate(vocab):
    
    print(pair)

"""##### Let's create a dictionary where the keys are the characters with some number assigned to them

* Using `dictionary comprehension`
"""

char_to_ind = {char:ind for ind,char in enumerate(vocab)}

char_to_ind

char_to_ind['H']

ind_to_char = np.array(vocab)

ind_to_char[33]

"""##### `Encoding the text`"""

encoded_text = np.array([char_to_ind[c] for c in text])

encoded_text

encoded_text.shape

"""* So we have almost 5.5 million characters"""

#

"""##### We now have a mapping we can use to go back and forth from characters to numerics"""

sample = text[:40]

sample

encoded_text[:40]

"""-------------------------------------------------------------------------------------------------------------------------------

### 3) Creating Batches

Overall what we are trying to achieve is to have the model predict the next highest probability character given a historical sequence of characters. Its up to us (the user) to choose how long that historic sequence. Too short a sequence and we don't have enough information (e.g. given the letter "a" , what is the next character) , too long a sequence and training will take too long and most likely overfit to sequence characters that are irrelevant to characters farther out. While there is no correct sequence length choice, you should consider the text itself, how long normal phrases are in it, and a reasonable idea of what characters/words are relevant to each other

* `Understand Text Sequences`

        To understand how the sequences are organized and shifted one character forward
        

* `Creating Batches`        


* `Shuffle Batches`

------------------------------------------------------------------------------------------------------------------------------

##### How long the training sequence should be?

##### We must make sure that our training sequences are long enough that they will actually grab the general structure of the text
"""

print(text[:500])

line = "From fairest creatures we desire increase"

len(line)

part_stanza = '''
From fairest creatures we desire increase,
  That thereby beauty's rose might never die,
  But as the riper should by time decease,
'''

len(part_stanza)

"""### `Training Sequences`

* The actual text data will be the text sequence shifted one character forward

* For Instance :

    * Sequence In : 'Hello my nam'
    
    * Sequence Out : 'ello my name'
"""

seq_len = 120   # choosing a value around (133)

'''
Total no.of sequences in the Text

// is to round off the division value

+1 is to include index_0

'''

total_num_seq = len(text) // (seq_len+1)

total_num_seq

"""##### Now let's create the Training sequences 

* `tf.data.Dataset.from_tensor_slices` function converts a text vector into a stream of character indices
"""

char_dataset = tf.data.Dataset.from_tensor_slices(encoded_text)

type(char_dataset)

for item in char_dataset.take(500):   # take method creates a dataset
    
    print(item)

for item in char_dataset.take(500):
    
    print(item.numpy())

for item in char_dataset.take(500):
    
    print(ind_to_char[item.numpy()])

"""##### So now let's create Sequences from it

* `batch` method converts the individual character calls into sequences we can feed in as a batch

* `drop_remainder` represents whether or not the last batch should be dropped in the case it has fewer elements than the actual `batch_size` elements; the default behaviour is not to drop the smaller batch
"""

sequences = char_dataset.batch(seq_len+1, drop_remainder=True)

"""So now we have our sequences, we will perform the following steps for each one of them to create our target text sequence :

* Grab the input text sequence

* Assign the target text sequence as the input text sequence shifted by one step forward

* Group them together as a tuple
"""

def create_seq_targets(seq):
    
    input_txt = seq[:-1]   # hello my nam
    
    target_txt = seq[1:]   # ello my name
    
    return input_txt, target_txt

"""##### Let's map the function to all the sequences 

So my final dataset will be :
"""

dataset = sequences.map(create_seq_targets)

for input_txt, target_txt in dataset.take(1):
    
    print(input_txt.numpy())
    
    print(''.join(ind_to_char[input_txt.numpy()]))
    
    print(target_txt.numpy())
    
    print(''.join(ind_to_char[target_txt.numpy()]))

"""* `So now we have all the actual sequences`


* `Let's create the training batches`


* `We have to shuffle those sequences into a random order, so the model doesn't learn on a particular ordering of the text. It should be any random snippet of the text and the model should start generating the next sequence from it`


* `By shuffling, the model doesn't overfit to any section of the text, but can instead generate characters given any seed text`
"""

batch_size = 128    # 128 sequences feeding into the network at a time

buffer_size = 10000

'''
buffer_size to shuffle the dataset so it doesn't attempt to shuffle the entire dataset in the memory

If dealing with the large dataset, it could cause a potential memory error

'''

dataset = dataset.shuffle(buffer_size).batch(batch_size, drop_remainder=True)

dataset

"""----------------------------------------------------------------------------------------------------------------------------
128 is the no.of sequences

each sequence is 120 long

first tuple is for input sequence

second tuple is for target sequence

-----------------------------------------------------------------------------------------------------------------------------------

------------------------------------------------------------------------------------------------------------------------------

### 4) Create the Model

* Set up the Loss Function

* Create the Model

    * Embedding layer
    
    * GRU layer
    
    * Dense layer
    
----------------------------------------------------------------------------------------------------------------------------

* Embeddings are the only way one can transform discrete feature into a vector form

* All machine learning algorithms take a vector and return a prediction

* Therefore if you have a categorical feature, the only way you can use it in a ML model is by embedding it into a vector

* The simplest kind of embedding is one-hot encoding:

        1 -> (1, 0, 0)
        2 -> (0, 1, 0)
        3 -> (0, 0, 1)

* We can replace the categorical feature with three possible values with the vectors as above without losing any information

* These vectors have as many elements as the number of values of the categorical feature

------------------------------------------------------------------------------------------------------------------------------

* When your categorical feature has a lot of possible values, it is often better to replace it with embeddings with lower dimensionality

* Lower dimensionality gives you two advantages:

        It is more computationally efficient, because smaller embeddings require less memory
        
        It regularizes your model, because the smaller number of parameters your model have, the better it is regularized
-------------------------------                

* Embeddings are often used to map words to vectors in NLP systems, words represented as vectors can be used as an input for recurrent neural network

* Refer this Article : [Word Embedding Layers with Keras](https://machinelearningmastery.com/use-word-embedding-layers-deep-learning-keras/)

* Embeddings are also quite often used in recommendation systems to represent high dimensionality categorical variables like user_id or recommendable_item_id

* Refer this Article : [Embeddings for Collaborative Filtering](https://developers.google.com/machine-learning/crash-course/embeddings/motivation-from-collaborative-filtering)

------------------------------------------------------------------------------------------------------------------------------


````sh
````

from keras.models import Sequential
from keras.layers import Embedding

import numpy as np

model = Sequential()

model.add(Embedding(1000, 4))    # 1000 words, 4 dimensions

model.compile(optimizer='adam', loss='mse')


print(model.predict(np.array([[4,8,3]])))


o/p :

     [[[-0.09090  -0.93339  0.87322  -0.90893]

       [-0.09989   0.89879 -0.97079   0.86853]
  
       [0.468687   0.78346  0.67352   0.42736]]]
  

-----------------------------------------------------------------------------------------------------------------------------

* The answer is that the embedding layers in TensorFlow completely differ from the the word embedding algorithms, such as word2vec and GloVe. They only share a similar name!


* Embedding refers to mapping a high-dimensional sparse feature vector to a dense vector with a much lower dimension. The embedding layer in TensorFlow is just like a look-up table. For instance, assume that there is a 2D tensor in which the first dimension represent the ID of a word and the second dimension represents the dense vector that is going to be learned during the training phase of the neural network. It is notable that you can also use pre-trained word embeddings (e.g., using word2vec) and use them as an input of the network. You can set “Trainable” argument to “False”, if you want to use pre-trained embeddings and do not wish to update them during the learning process of your network

-------------------------------------------------------------------------------------------------------------------------------
------------------------------------------------------------------------------------------------------------------------------

* We based this model architecture off the [DeepMoji](https://deepmoji.mit.edu/) and the original source code can be found [here](https://github.com/bfelbo/DeepMoji)


* The embedding layer will serve as the input layer, which essentially creates a lookup table that maps the numbers indices of each character to a vector with "embedding dim" number of dimensions. As you can imagine, the larger this embedding size, the more complex the training. This is similar to the idea behind word2vec, where words are mapped to some n-dimensional space. Embedding before feeding straight into the LSTM usually leads to more realisitic results
"""

vocab_size = len(vocab)            # unique characters

embed_dim = 64                     # near to vocab_size (prefer to be lesser than vocab_size)

rnn_neurons = 1026

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, GRU, Dense

"""##### `Let's create a function that easily adapts to different variables as shown above :`

### Setting up Loss Function :

* For our loss we will use `sparse categorical crossentropy`, which we can import from Keras. We will also set this as logits=True

-----------------------------------------------------------------------------------------------------------
###### Sparse Categorical CrossEntropy vs Categorical CrossEntropy

* `If your targets are one-hot encoded, use categorical_crossentropy. Examples of one-hot encodings :`

                [1,0,0]

                [0,1,0] 

                [0,0,1]
                

* `But if your targets are integers, use sparse_categorical_crossentropy. Examples of integer encodings :`

                1
                
                2
                
                3
"""

from tensorflow.keras.losses import sparse_categorical_crossentropy

# help(sparse_categorical_crossentropy)

"""* We can't just pass-in sparse_c_entropy because we have to add logits=True, as we have one-hot encoded if it is False they are not One-hot encoded 


* As we need to add this, we have to create our own custom function as follows :
"""

def sparse_cat_loss(y_true, y_pred):
    
    return sparse_categorical_crossentropy(y_true, y_pred, from_logits=True)

"""##### `Adaptable Model Function :`"""

def create_model(vocab_size, embed_dim, rnn_neurons, batch_size) :
    
    model = Sequential()
    
    model.add(Embedding(vocab_size, embed_dim, batch_input_shape=[batch_size, None]))
    
    model.add(GRU(rnn_neurons, return_sequences=True, stateful=True, recurrent_initializer='glorot_uniform'))
    
    model.add(Dense(vocab_size))
    
    model.compile(optimizer='adam', loss=sparse_cat_loss)
    
    return model

"""* return_sequences - to include even the last sequence

* stateful - to keep the current state

* recu_intializer - weight values for the layer
"""

model = create_model(vocab_size=vocab_size, embed_dim=embed_dim, rnn_neurons=rnn_neurons, batch_size=batch_size)

model.summary()

"""------------------------------------------------------------------------------------------------------------------------------

### 5) Train the Model

* `Let's make sure that everything is ok with our model before we spend too much time on training`


* `So let's pass in a batch to confirm that the model predicts some random characters without any training`

* `So let's run an input batch`
"""

for input_example_batch, target_example_batch in dataset.take(1):

  # predict off some random batch
  example_batch_predictions = model(input_example_batch)

"""* `input_example_batch` is the original sequence and `target_example_batch` is the original sequence shifted forward by 1 character"""

example_batch_predictions.shape

example_batch_predictions[0]              # grabbing the very first batch predictions

"""##### These values are just probabilities that our model assumes for each concurrent character"""

sampled_indices = tf.random.categorical(example_batch_predictions[0],num_samples=1)

sampled_indices

"""##### Inorder to pass this sort of Array to ind_to char sequence, we need to reshape this Array"""

sampled_indices = tf.squeeze(sampled_indices, axis=-1).numpy()            # Reformat to not to be a list of lists

sampled_indices             # Now we have got this in the format of ind_to_char sequence

ind_to_char[sampled_indices]

"""##### The above values are just a bunch of random characters since our model has not been trained at all

##### `After confirming the dimensions are working, we can now train our model :`
"""

epochs = 30            # 30 epochs atleast to get the realistic results

model.fit(dataset, epochs=epochs)

"""----------------------------------------------------------------------------------------------------------------------------------------------------------------

### 6) Generating Text
"""

model.save('shakespeare.h5')

"""##### `Currently our model only expects 128 sequences at a time. We can create a new model that only expects a batch_size=1`

##### `We can create a new model with this batch_size, then load our saved model's weights. Then call .build() on the model`
"""

from tensorflow.keras.models import load_model

model = create_model(vocab_size, embed_dim, rnn_neurons, batch_size=1)

model.load_weights('shakespeare.h5')

model.build(tf.TensorShape([1, None]))      # we will build the model by passing the input shape

model.summary()

"""##### `Notice that it is the same model summary as earlier but instead of 128 as the batch size now it only expects a batch size of 1`

##### Now we can create our own custom function to generate the text
"""

def generate_text(model, start_seed, gen_size=500, temp=1.0) :

  '''
  model : Trained model to generate text

  start_seed : Initial seed text in string form

  gen_size : No.of characters to generate

  temp : hyper-parameter  used to control the randomness of predictions by scaling the logits before applying softmax

  ----------------
  
  logits are the values to be used as inputs to softmax

  sigma ^ -1 (x) is called as logit in statistics, and it stands for the inverse function of logistic sigmoid function

  -----------------

  Basic idea behind this function is to take in some seed text, 

  format it so that it is in the correct shape for our network

  Then loop the sequence as we keep adding our own predicted characters.

  Pretty similar to the work in the RNN time series analysis

  '''

  num_generate = gen_size

  # Vectorizing starting seed text
  input_eval = [char_to_ind[s] for s in start_seed]             # for every character will go ahead and transform it to an index and then we will have them in a list

  # Expand this to match batch format shape
  input_eval = tf.expand_dims(input_eval, 0)

  # Empty List to hold the resulting generated text
  text_generated = []


  # Temperature effects randomness in our resulting text
  # The term is derived from entropy/thermodynamics.
  # The temperature is used to effect probability of next characters.
  # Higher probability == lesss surprising/ more expected
  # Lower temperature == more surprising / less expected

  temperature = temp

  # Here batch size == 1
  model.reset_states()

  for i in range(num_generate):

    # Generate Predictions
    predictions = model(input_eval)

    # Remove the batch shape dimension
    predictions = tf.squeeze(predictions, 0)     # just reverse of expand_dims

    # Use a Categorical distribution to select the next character

    predictions = predictions/temperature

    predicted_id = tf.random.categorical(predictions, num_samples=1)[-1, 0].numpy()

    # Pass the predicted character for the next input
    input_eval = tf.expand_dims([predicted_id], 0)

    # Transform back to character letter
    text_generated.append(ind_to_char[predicted_id])

  return (start_seed + ''.join(text_generated))

print(generate_text(model, 'flower', gen_size=1000))

